{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HAVE_CUDA=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if HAVE_CUDA else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) # 1 inp channel, 10 output channels\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5) # \n",
    "        self.conv2_drop = nn.Dropout2d() \n",
    "\n",
    "        \n",
    "        self.fc1 = nn.Linear(320, 100)\n",
    "        self.fc21 = nn.Linear(100, 20)\n",
    "        self.fc22 = nn.Linear(100, 20)\n",
    "        self.fc3 = nn.Linear(20, 100)\n",
    "        self.fc4 = nn.Linear(100, 784)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "          std = logvar.mul(0.5).exp_()\n",
    "          eps = Variable(std.data.new(std.size()).normal_())\n",
    "          return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "          return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        return self.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        mu, logvar = self.encode(x.view(-1, 320))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE()\n",
    "if HAVE_CUDA:\n",
    "    model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784))\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    # Normalise by same number of elements as in reconstruction\n",
    "    KLD /= batch_size * 784\n",
    "\n",
    "    return BCE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        if HAVE_CUDA:\n",
    "            data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data[0] / len(data)))\n",
    "\n",
    "    print('====> TRAIN Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "    loss_val =  train_loss/len(train_loader.dataset)\n",
    "    return loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for i, (data, _) in enumerate(test_loader):\n",
    "        if HAVE_CUDA:\n",
    "            data = data.cuda()\n",
    "        data = Variable(data, volatile=True)\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        test_loss += loss_function(recon_batch, data, mu, logvar).data[0]\n",
    "        if i == 0:\n",
    "          n = min(data.size(0), 8)\n",
    "          comparison = torch.cat([data[:n],\n",
    "                                  recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "          save_image(comparison.data.cpu(),\n",
    "                     'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.001237\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.001205\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.001158\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.001194\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.001206\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.001176\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.001178\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.001159\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.001182\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.001208\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.001187\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.001178\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.001184\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.001171\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.001159\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.001172\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.001152\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.001161\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.001210\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.001152\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.001139\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.001156\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.001227\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.001107\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.001194\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.001179\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.001134\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.001176\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.001148\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.001182\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.001187\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.001142\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.001123\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.001156\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.001156\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.001164\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.001179\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.001111\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.001196\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.001152\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.001156\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.001155\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.001131\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.001209\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.001122\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.001120\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.001136\n",
      "====> TRAIN Epoch: 1 Average loss: 0.0012\n",
      "train_loss 0.0011664077001313368\n",
      "====> Test set loss: 0.0011\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.001195\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.001165\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.001157\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.001132\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.001079\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.001197\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.001175\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.001150\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.001115\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.001140\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.001178\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.001135\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.001154\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.001122\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.001174\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.001092\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.001157\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.001158\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.001144\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.001148\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.001173\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.001158\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.001097\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.001171\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.001170\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.001181\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.001169\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.001097\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.001173\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.001132\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.001120\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.001135\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.001136\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.001161\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.001186\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.001197\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.001148\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.001156\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.001145\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.001210\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.001135\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.001134\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.001159\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.001158\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.001143\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.001167\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.001070\n",
      "====> TRAIN Epoch: 2 Average loss: 0.0012\n",
      "train_loss 0.0011523313182095686\n",
      "====> Test set loss: 0.0011\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.001138\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.001147\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.001147\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.001132\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.001116\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.001174\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.001141\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.001153\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.001159\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.001180\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.001118\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.001152\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.001139\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.001178\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.001146\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.001209\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.001201\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.001128\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.001174\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.001178\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.001154\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.001094\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.001139\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.001173\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.001104\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.001140\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.001099\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.001150\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.001139\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.001131\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.001153\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.001173\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.001095\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.001109\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.001139\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.001147\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.001078\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.001138\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.001164\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.001118\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.001132\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.001118\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.001171\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.001140\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.001135\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.001129\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.001164\n",
      "====> TRAIN Epoch: 3 Average loss: 0.0011\n",
      "train_loss 0.0011423587044080098\n",
      "====> Test set loss: 0.0011\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.001126\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.001167\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.001121\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.001112\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.001181\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.001072\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.001138\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.001109\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.001095\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.001148\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.001146\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.001164\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.001082\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.001145\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.001111\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.001141\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.001150\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.001130\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.001167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.001097\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.001151\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.001132\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.001171\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.001161\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.001134\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.001090\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.001102\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.001103\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.001094\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.001126\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.001097\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.001142\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.001129\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.001095\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.001113\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.001101\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.001109\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.001144\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.001159\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.001145\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.001139\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.001145\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.001127\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.001124\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.001137\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.001171\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.001062\n",
      "====> TRAIN Epoch: 4 Average loss: 0.0011\n",
      "train_loss 0.0011337520415584247\n",
      "====> Test set loss: 0.0011\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.001151\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.001148\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.001147\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.001126\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.001135\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.001130\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.001079\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.001106\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.001147\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.001114\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.001129\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.001117\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.001121\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.001127\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.001156\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.001124\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.001163\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.001117\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.001137\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.001123\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.001187\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.001080\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.001117\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.001133\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.001098\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.001149\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.001140\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.001104\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.001093\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.001075\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.001124\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.001144\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.001108\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.001158\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.001135\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.001137\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.001118\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.001173\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.001161\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.001144\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.001057\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.001109\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.001147\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.001099\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.001132\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.001145\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.001103\n",
      "====> TRAIN Epoch: 5 Average loss: 0.0011\n",
      "train_loss 0.0011272411088148753\n",
      "====> Test set loss: 0.0011\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.001106\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.001082\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.001110\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.001100\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.001148\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.001154\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.001086\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.001167\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.001120\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.001148\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.001104\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.001166\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.001106\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.001136\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.001127\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.001156\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.001140\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.001071\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.001102\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.001165\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.001124\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.001148\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.001124\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.001138\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.001112\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.001103\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.001132\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.001111\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.001122\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.001151\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.001153\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.001106\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.001143\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.001082\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.001131\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.001111\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.001120\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.001152\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.001159\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.001152\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.001122\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.001114\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.001122\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.001080\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.001125\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.001126\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.001096\n",
      "====> TRAIN Epoch: 6 Average loss: 0.0011\n",
      "train_loss 0.0011210897276798884\n",
      "====> Test set loss: 0.0011\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.001107\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.001120\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.001111\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.001110\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.001101\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.001097\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.001114\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.001114\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.001107\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.001153\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.001098\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.001101\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.001157\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.001110\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.001097\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.001104\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.001128\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.001081\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.001101\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.001071\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.001158\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.001101\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.001103\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.001119\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.001107\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.001109\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.001128\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.001105\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.001123\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.001112\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.001086\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.001097\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.001100\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.001094\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.001142\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.001102\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.001156\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.001091\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.001117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.001120\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.001155\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.001144\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.001131\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.001144\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.001078\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.001128\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.001123\n",
      "====> TRAIN Epoch: 7 Average loss: 0.0011\n",
      "train_loss 0.0011154718955357868\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.001086\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.001153\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.001110\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.001100\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.001151\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.001077\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.001103\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.001054\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.001151\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.001086\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.001136\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.001066\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.001119\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.001153\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.001105\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.001108\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.001127\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.001128\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.001040\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.001111\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.001133\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.001120\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.001137\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.001158\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.001131\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.001119\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.001100\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.001164\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.001151\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.001136\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.001111\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.001071\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.001092\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.001118\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.001121\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.001120\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.001130\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.001082\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.001106\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.001178\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.001150\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.001083\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.001103\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.001101\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.001119\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.001112\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.001101\n",
      "====> TRAIN Epoch: 8 Average loss: 0.0011\n",
      "train_loss 0.0011118265862266224\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.001129\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.001106\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.001116\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.001134\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.001104\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.001091\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.001105\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.001108\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.001123\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.001121\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.001093\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.001096\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.001140\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.001120\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.001118\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.001113\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.001141\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.001156\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.001117\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.001125\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.001091\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.001074\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.001119\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.001129\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.001155\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.001143\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.001100\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.001076\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.001096\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.001104\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.001089\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.001112\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.001077\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.001125\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.001125\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.001095\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.001122\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.001098\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.001120\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.001146\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.001105\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.001119\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.001131\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.001110\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.001093\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.001110\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.001094\n",
      "====> TRAIN Epoch: 9 Average loss: 0.0011\n",
      "train_loss 0.0011078688845038414\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.001123\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.001100\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.001122\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.001181\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.001133\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.001040\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.001138\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.001037\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.001114\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.001122\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.001096\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.001118\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.001108\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.001077\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.001072\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.001102\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.001092\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.001120\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.001105\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.001088\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.001167\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.001111\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.001114\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.001132\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.001121\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.001082\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.001122\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.001065\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.001085\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.001066\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.001143\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.001113\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.001153\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.001110\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.001118\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.001123\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.001099\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.001134\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.001110\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.001133\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.001134\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.001100\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.001102\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.001102\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.001099\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.001091\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.001142\n",
      "====> TRAIN Epoch: 10 Average loss: 0.0011\n",
      "train_loss 0.0011049945468703905\n",
      "====> Test set loss: 0.0010\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(1, 1 + 10):\n",
    "    train_loss = train(epoch)\n",
    "    print('train_loss', train_loss)\n",
    "    losses.append(train_loss)\n",
    "    test(epoch)\n",
    "    sample = Variable(torch.randn(64, 20))\n",
    "    if HAVE_CUDA:\n",
    "       sample = sample.cuda()\n",
    "    sample = model.decode(sample).cpu()\n",
    "    save_image(sample.data.view(64, 1, 28, 28),\n",
    "               'results/sample_' + str(epoch) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0011664077001313368, 0.0011523313182095686, 0.0011423587044080098, 0.0011337520415584247, 0.0011272411088148753, 0.0011210897276798884, 0.0011154718955357868, 0.0011118265862266224, 0.0011078688845038414, 0.0011049945468703905]\n"
     ]
    }
   ],
   "source": [
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc4dd2c2518>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAD8CAYAAAC2PJlnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMVYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc555a745f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
